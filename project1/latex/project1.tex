\begin{abstract}
    Conjugate gradient algorithm... Laplacian operator... 
\end{abstract}

\section{Introduction}

\iffalse
Interesting, cogent account of the work in this report
> in the context of the mathematical problem considered 
> in the scientific computing more widely
Very well referenced.

Talk about the problems we are going to discuss
> Laplacian operator. Finite difference of the finite difference of Laplace
> How do we get the matrix?
> Both for 1 and 2D.

Top 10 algorithms 20th century. How does it relate to wider mathematical community?

\fi

Conjugate gradient is known as a Krylov subspace method. 
In 2000, this class of methods was voted third in the top 10 algorithms of the 20th century.

It was developed by Hestenes and Steifel.
Can be thought of as a direct method or an iterative method.
Was not adopted initially as a direct method.
Much more powerful when performed iteratively.
Only works for symmetric and positive definite matrices.

Problem we are looking at is Poisson problem in 1 and 2 dimensions.
This is the Laplace equation with nonzero right hand side, which consists of second order derivatives with respect to indepdendent variables.
Elliptic PDE whose solutions are harmonic functions

In the discretisation of the Poisson problem, we obtain a linear system of equations.
This system is symmetric and so can be solved using cg.

Laplace's equation features in electrostatics, fluid dynamics, and several other areas of physics.
Thus efficient methods for solving linear systems involving this problem are desired.




\section{Mathematics}

\iffalse
CG discussion:

Main algorithm
Mathematics of the mechanism
Preconditioning
Limitations of the algorithm
How do we get good/bad convergence?
Proof of convergence
Proof of complexity
\fi

\subsection{Conjugate Gradient}
\label{sec:cg}

% Steepest descent vs conjugate gradient
% Definition of A-conjugate vectors


% Minimisation of function <=> Solution to Ax=b
% Pos-def => convex => local min is global min
% Begin with initial guess, find min of f -> get sol to Ax=b

% Motivation of algorithm
Consider the problem of minimising the function
\[
    f(x) = \frac{1}{2} x^T A x - b^T x,
\]
with $A$ symmetric and positive definite. This is minimised exactly when 
\[
    \nabla f(x) = A x - b = 0.
\]
Hence, the solution to $Ax=b$ is the minimiser of $f(x)$.
Consider the Hessian of $f(x)$,
\[
    \nabla^2 f(x) = A.
\]
Since $A$ is positive definite, this means that $f(x)$ is a convex function.
Convexivity gives us that any local minima is in fact a global minimum.
Hence, by minimising $f(x)$, we find the solution to the linear system.

% Discussion of algorithm
Two vectors $u, v$ are conjugate if $u^T v = 0$. 
Two vectors are said to be $A$-orthogonal or conjugate with respect to $A$ if $u^T A v = 0$.
Conjugate vectors then form a linearly independent set.
The CG algorithm selects $A$-orthogonal search directions and finds the optimal step size in that direction.

This has the benefit of exclusively searching along previously unsearched directions and for the optimal search step for the solution.
Contrast this with steepest descent, which has the possibility of overshooting the solution.
Thus we can expect convergence to be much more rapid when these two methods are compared.


\subsection{Rate of convergence}

For certain matrices, $A$, we can expect immediate convergence. single eigenvalue
In the case that 


\subsection{Different Matrices}

\iffalse

For the matrix A dependent on m, the convergence of cg breaks in general.
Since A is not symmetric, the conditions for convergence do not hold.
However, for large values of m, we may still find convergence for the problem.

If m is large, we have "strong" diagonal dominance. 
So we are effectively solving a diagonal system.
b is a constant vector, so x can be approximated by 2.5/a_{ii}.
If m is small, approximately 0, no convergence at all.
How does this play into cg algorithm?

We need symmetry and pos defness for convergence.
May not have set of linearly independent eigenvectors
So the solution will not converge


\fi

\section{Implementation}

\iffalse
Implementation discussion:

+Better band storage for wide bands with lots of zeros. Store the nonzeros with their locations.
-Better interaction with the program for inputting data or getting results. Use of a scripting language and parser. 
+Should treat vectors as matrices, reduces number of classes, fewer bugs and easier implementation.
+Sparse matrices
Unit tests
Error handling
\fi

\subsection{Matrix Classes}

The implementation of the \inline{MVector}, \inline{MMatrix}, and \inline{MBandedMatrix} classes can be found in the code listings in the appendix.

% VECTORS AND MATRICES
These classes behave as we would expect from the standard mathematical objects. 
This had been completed by overloading the C++ operators, \inline{+, -, *, /}, to give reasonable behaviour to the classes.
The underlying data structure for all of these classes is a \inline{vector<double>} object.
Accessing elements of a \inline{MVector} is identical to accessing elements of the underlying \inline{vector<double>}.
Accessing elements of a \inline{MMatrix} requires converting between matrix coordinates and the list coordinates of the \inline{vector}.
In matrix coordinates, $(i,j)$ specifies the element at the $i^\mathrm{th}$ row and $j^\mathrm{th}$ column.
To convert to list coordinates, observe that $j$ is the offset from the start of the row and $i$ specifies the number of rows down.
If each row consists of $n$ elements, i.e. the number of columns, then we have in list coordinates, $(i,j) \mapsto i*n + j$.


% VECTOR IS DISTINCT FROM THE MATRIX
The vector class is created distinct from the matrix class.
Although vector and matrix objects interact with each other in the expected way, a representation of a vector $x$ as a matrix in $x\in\R^{n\times 1}$ is also possible.
In this case, a vector $x$ could be instantiated as \inline{MMatrix x(n,1);}.
Then the product of a vector and matrix is defined as long as the corresponding dimensions match, so it would be necessary to consider the dimensions of matrices in the necessary operations.
By making this change, we reduce duplicate code as we can reuse the code for stream outputs, matrix-matrix multiplication, and scalar-matrix multiplication.
Ideally, code reuse leads to fewer bugs as fewer classes and lines of code are involved in the project.


% SPARSITY, BANDED MATRIX
The \inline{MBandedMatrix} implementation for a banded matrix behaves identically to an \inline{MMatrix} in the case that both are banded matrices.
The difference lies solely in that the structure matters for the \inline{MBandedMatrix}.
For a matrix $B\in\R^{n\times n}$ with $l$ lower bands and $r$ upper bands, we define a new matrix $B^*\in\R^{n \times (l+r+1)}$ whose elements are
\[
    b^*_{i,j+l-i} = b_{i,j}.
\]
The \inline{MBandedMatrix} allows us to exploit the sparse structure of banded matrices.
In particular, a matrix whose entries are close to the diagonal are most well represented by this class. 
If $l+r+1 << n$, then we benefit from greatly reduced storage costs.
Matrices whose bandwidths are large and contain many zero elements between nonzeros on the same row suffer from the storage of several zeros.
In this case, the cost savings from using an \inline{MBandedMatrix} may not be sufficient to justify its use.

For example, the $n\times n$ 1 dimensional discrete Laplacian operator is a tridiagonal matrix. 
This has 1 upper band, 1 lower band, as well as entries on the diagonal.
So storing this using a \inline{MBandedMatrix} requires $3n$ entries, using $3/n$ less space for storage.
Compare this with the $n^2\times n^2$ 2 dimensional discrete Laplacian operator, which only has 5 nonzero diagonals, but contain $n-2$ zero entries between the nonzeros.
Using our storage scheme, we require storage of $n^2(2n+1)=2n^3+n^2$ entries. 
Storing only the nonzero diagonals, we would need to store $5n^2$ entries.
From this we see for large values of $n$, this storage format is space-inefficient in certain cases and can be optimised.

Alternative sparse storage methods may acheive improved storage savings in cases where bandwidth is large and consists of many zero entries.
For example, the \textit{compressed sparse row} format involves storage of three arrays consisting of the nonzero elements of a matrix as well as index information.
The first array contains the nonzero elements of the matrix, while the second two arrays contain row and column index information for the entries in the first array.
The format is compressed row, because redundant row indices are ommitted in favour of a method describing the number of elements on each row.
This approach to storing sparse matrices is similar to that which is used by MATLAB and other linear algebra packages handling sparse matrices.
% REFERENCE

Another approach to reduce storage would be to use the properties of matrix symmetry. 
For a symmetric matrix, the elements above and below the diagonal are identical, and so there is no need to store these elements.
For a banded matrix, this translates to storing only one of the upper or lower bands.
An $n\times n$ \inline{MMatrix} can be compressed to an $n \times (l+r+1)$ \inline{MBandedMatrix}.
Exploiting symmetry, we can reduce this to storing $n \times (r+1)$ elements.


\subsection{Conjugate Gradient}

The main algorithm of study in this project is the conjguate gradient method for solving symmetric, positive definite linear systems iteratively.
Following algorithm \ref{alg:cg}, as explained in section \ref{sec:cg}, the implementation is straightforward.
After initialisation of the variables, we begin our iteration stage. 
Given a maximum number of iterations and a tolerance, we guarantee that iteration has a fixed endpoint.
This will be the first of either the $L^{2}$ norm of the residual is below the tolerance of the maximum iteration count is reached.

In order to avoid recomputing dot products and matix-vector products, certain variables are used for storage and reuse.
For example, we use $r_k \cdot r_k$ twice and $A p_k$ twice. By storing these results we avoid multiple computations.


\section{Results}

\iffalse
Results:
Graphs, errors, convergence rates,
\fi


\section{Conclusion}

\iffalse
    Conclusion draws together strands of work throughout the report, 
    demonstrating how the results obtained relate to each other and 
    the broader context of the mathematical problem.

Discussion of results in context of problem
Talk about related algorithms
> BiCG
> CG squared
Preconditioning
Parallelisation, optimisations
Results
\fi

\clearpage
\appendix
\section{Tables}


\section{Algorithms}

% Conjugate gradient algorithm

\begin{algorithm}[H]
\KwData{Symmetric, positive definite matrix $A\in\R^{n\times n}$}
\KwData{Data vector $b\in\R^n$}
\KwResult{Vector $x$, solving $Ax=b$}



\caption{Conjugate Gradient}
\label{alg:cg}
\end{algorithm}


\section{Code Listings}
