% https://www.siam.org/meetings/la09/talks/oleary.pdf
% http://www2.math.ethz.ch/education/bachelor/seminars/fs2008/nas/filimon.pdf
% http://epubs.siam.org/doi/pdf/10.1137/1031003
% http://www.sam.math.ethz.ch/~mhg/pub/biksm.pdf

\begin{abstract}
    Conjugate gradient algorithm... Laplacian operator... 
\end{abstract}

\section{Introduction}

\iffalse
Interesting, cogent account of the work in this report
> in the context of the mathematical problem considered 
> in the scientific computing more widely
Very well referenced.

Talk about the problems we are going to discuss
> Laplacian operator. Finite difference of the finite difference of Laplace
> How do we get the matrix?
> Both for 1 and 2D.

Top 10 algorithms 20th century. How does it relate to wider mathematical community?

\fi

Krylov subspace methods solve the linear system $Ax=b$ by building information about the solution from the span of the shifted Krylov subspace 
\[
    \mathcal{K}_k(A, r^{(0)}) = \mathrm{span}\left( r^{(0)}, Ar^{(0)}, A^2r^{(0)}, \ldots, A^{k-1}r^{(0)} \right),
\]
where $r^{(0)} = A - bx^{(0)}$ for some initial guess $x^{(0)}$.
In 2000, this class of methods was listed in the top 10 algorithms whose influence has been greatest on science and engineering of the 20th century \cite{top10}. 
Along with other methods, such as the steepest descent, Arnoldi, and Lanczos algorithms, the conjugate gradient method shares a highly regarded spot in mathematics.

The conjgate gradient algorithm was primarily developed by Hestenes and Steifel and was part of a number of important computational innovations at the time \cite{cghist}.
Originally, the algorithm was a direct method, and this was a cause of the algorithm's lack of initial adoption.
% When viewed as an iterative method, the algorithm is theoretically identical to the direct method but converges in fewer iterations 
Although, conjugate gradient can be used as both a direct and iterative solver, it is much more powerful when performed iteratively, and performs well for sparse matrices.
It should be noted that the method requires the matrix $A$ to be symmetric and positive definite, however modifications of the algorithm remove these constraints.
For example, the biconjugate gradient stabilised algorithm may be used on nonsymmetric problems. 

One of the problems we will be using to test our implementation is the discretisation of the Poisson problem in 1 and 2 dimensions.
The Poisson problem is the Laplace equation with nonzero right hand side, which consists of second order derivatives with respect to the indepdendent variables.
This is an elliptic PDE whose solutions are harmonic functions. 
The equation features in electrostatics, fluid dynamics, and several other areas of physics.
For example, in heat conduction, the equation represents the steady-state heat equation.
The discretisation leads to a symmetric and positive definite system of linear equations, and so the conjugate gradient algorithm is an appropriate solver in this case.
Thus the study of efficient methods for solving linear systems involving this problem are desired.

In this project, we implement matrix and vector classes in C++ in order to implement and study the conjugate gradient algorithm.
In this report, we discuss the mathematics of the conjugate gradient algorithm, as well as discuss the results of the application of the algorithm on different problems.


\section{Conjugate Gradient}
\label{sec:cg}

\iffalse
CG discussion:

Main algorithm
Mathematics of the mechanism
Preconditioning
Limitations of the algorithm
How do we get good/bad convergence?
Proof of convergence
Proof of complexity
\fi

% Steepest descent vs conjugate gradient
% Definition of A-conjugate vectors

Consider the problem of minimising the function
\[
    f(x) = \frac{1}{2} x^T A x - b^T x,
\]
with $A$ symmetric and positive definite. This is minimised exactly when 
\[
    \nabla f(x) = A x - b = 0.
\]
The Hessian of $f(x)$ is
\[
    \nabla^2 f(x) = A,
\]
and since $A$ is positive definite, this means that $f(x)$ is a convex function.
Convexivity gives us that any local minimum is a global minimum.
Hence, the solution to $Ax=b$ is the minimiser of $f(x)$.
So by minimising $f(x)$, we will converge to the global minimum and find the solution to the linear system.

% Discussion of algorithm
Two vectors are said to be $A$-orthogonal or conjugate with respect to $A$ if $\langle u,v \rangle = u^T A v= 0$.

Starting with an initial guess, $x^{0}$, the CG algorithm builds
The method constructs mutually conjugate vectors from the residuals and finds the optimal step size along that vector to search for the solution.
This is done by constructing the vectors using Gram-Schmidt orthogonalisation.
At step $k$, we have a set of mutually conjugate vectors $U=\{p_0, p_1,\ldots,p_k\}$, that is, they are pairwise orthogonal with respect to the inner product $\langle u,v \rangle$.
To create the vector $p_{k+1}$ conjugate to all the other vectors in $U$, we compute the orthogonal projection of the residual at step $k$ onto the subspace $U$.
% This involves $v$ is created orthgonal to by projecting 
By removing the requirement the search directions be mutually conjugate, we obtain the gradient descent algorithm.

This has the benefit of exclusively searching along previously unsearched directions and for the optimal search step for the solution.
Contrast this with steepest descent, which has the possibility of overshooting the solution.
Thus we can expect convergence to be much more rapid when these two methods are compared.


The main algorithm of study in this project is the conjguate gradient method for solving symmetric, positive definite linear systems iteratively.
Following the algorithm described in the handout, and as explained in section \ref{sec:cg}, the implementation is straightforward.
After initialisation of the variables, we begin our iteration stage. 
Given a maximum number of iterations and a tolerance, we guarantee that iteration has a fixed endpoint.
This will be the first of either the $L^{2}$ norm of the residual is below the tolerance of the maximum iteration count is reached.

In order to avoid recomputing dot products and matix-vector products, certain variables are used for storage and reuse.
For example, we use $r_k \cdot r_k$ twice and $A p_k$ twice. By storing these results we avoid multiple computations.


\subsection{Rate of convergence}

For certain matrices, $A$, we can expect immediate convergence.
Consider the case where the matrix $A$ is symmetric and has a single eigenvalue, $\lambda>0$.



\section{Matrix Classes}
\label{sec:classes}

The implementation of the \inline{MVector}, \inline{MMatrix}, and \inline{MBandedMatrix} classes can be found in  section \ref{sec:code} in code listings \ref{lst:mvector}, \ref{lst:mmatrix}, and \ref{lst:mbandedmatrix}, respectively.

\subsection{The \inline{MVector} class}

The vector class is created distinct from the matrix class.
Although vector and matrix objects interact with each other in the expected way, a representation of a vector $x$ as a matrix in $x\in\R^{n\times 1}$ is also possible.
In this case, a vector $x$ could be instantiated as \inline{MMatrix x(n,1);}.
Then the product of a vector and matrix is defined as long as the corresponding dimensions match, so it would be necessary to consider the dimensions of matrices in the necessary operations.
By making this change, we reduce duplicate code as we can reuse the code for stream outputs, matrix-matrix multiplication, and scalar-matrix multiplication.
Ideally, code reuse leads to fewer bugs as fewer classes and lines of code are involved in the project.

\subsection{The \inline{MMatrix} class}

% VECTORS AND MATRICES
These classes behave as we would expect from the standard mathematical objects. 
This had been completed by overloading the C++ operators, \inline{+, -, *, /}, to give reasonable behaviour to the classes.
The underlying data structure for all of these classes is a \inline{vector<double>} object.
Accessing elements of a \inline{MVector} is identical to accessing elements of the underlying \inline{vector<double>}.
Accessing elements of a \inline{MMatrix} requires converting between matrix coordinates and the list coordinates of the \inline{vector}.
In matrix coordinates, $(i,j)$ specifies the element at the $i^\mathrm{th}$ row and $j^\mathrm{th}$ column.
To convert to list coordinates, observe that $j$ is the offset from the start of the row and $i$ specifies the number of rows down.
If each row consists of $n$ elements, i.e. the number of columns, then we have in list coordinates, $(i,j) \mapsto i*n + j$.


\subsection{The \inline{MBandedMatrix} class}

The \inline{MBandedMatrix} implementation for a banded matrix behaves identically to an \inline{MMatrix} in the case that both are banded matrices.
The difference lies solely in that the structure matters for the \inline{MBandedMatrix}.
For a matrix $B\in\R^{n\times n}$ with $l$ lower bands and $r$ upper bands, we define a new matrix $B^*\in\R^{n \times (l+r+1)}$ whose elements are
\[
    b^*_{i,j+l-i} = b_{i,j}.
\]
The \inline{MBandedMatrix} allows us to exploit the sparse structure of banded matrices.
In particular, a matrix whose entries are close to the diagonal are most well represented by this class. 
If $l+r+1 << n$, then we benefit from greatly reduced storage costs.
Matrices whose bandwidths are large and contain many zero elements between nonzeros on the same row suffer from the storage of several zeros.
In this case, the cost savings from using an \inline{MBandedMatrix} may not be sufficient to justify its use.

For example, the $n\times n$ 1 dimensional discrete Laplacian operator is a tridiagonal matrix. 
This has 1 upper band, 1 lower band, as well as entries on the diagonal.
So storing this using a \inline{MBandedMatrix} requires $3n$ entries, using $3/n$ less space for storage.
Compare this with the $n^2\times n^2$ 2 dimensional discrete Laplacian operator, which only has 5 nonzero diagonals, but contain $n-2$ zero entries between the nonzeros.
Using our storage scheme, we require storage of $n^2(2n+1)=2n^3+n^2$ entries. 
Storing only the nonzero diagonals, we would need to store $5n^2$ entries.
From this we see for large values of $n$, this storage format is space-inefficient in certain cases and can be optimised.

Alternative sparse storage methods may acheive improved storage savings in cases where bandwidth is large and consists of many zero entries.
For example, the \textit{compressed sparse row} format involves storage of three arrays consisting of the nonzero elements of a matrix as well as index information.
The first array contains the nonzero elements of the matrix, while the second two arrays contain row and column index information for the entries in the first array.
The format is compressed row, because redundant row indices are ommitted in favour of a method describing the number of elements on each row.
This approach to storing sparse matrices is similar to that which is used by MATLAB and other linear algebra packages handling sparse matrices.
% REFERENCE

Another approach to reduce storage would be to use the properties of matrix symmetry. 
For a symmetric matrix, the elements above and below the diagonal are identical, and so there is no need to store these elements.
For a banded matrix, this translates to storing only one of the upper or lower bands.
An $n\times n$ \inline{MMatrix} can be compressed to an $n \times (l+r+1)$ \inline{MBandedMatrix}.
Exploiting symmetry, we can reduce this to storing $n \times (r+1)$ elements.


\section{Results}

\subsection{Rates of convergence}



\subsection{Comparison of matrix classes}
\def\mm{\inline{MMatrix}}
\def\mb{\inline{MBandedMatrix}}

The implementation of the matrix classes \mm and \mb can be found in listings \ref{lst:mmatrix} and \ref{lst:mbandedmatrix}, respectively.
A discussion of the implementation can be found in section \ref{sec:classes}.



Firstly, we should expect that the solution generated by these classes would be the same.
The \mb is only a reduced memory representation of the \mm, and so no additional activity should exist beyond element access and storage.
Indeed, we find this to be the case in tests and in plots comparing the iterations to converge, such as figure \ref{fig:2d-matrix-type-compare-iter}, where the number of iterations for each problem size coincide for both classes.

The only computations involving a matrix in the CG algorithm are matrix-vector products.
A standard matrix-vector product consists of $O(n^2)$ operations.
As a costly and frequently used operation, the implementation would benefit from any reduction of this cost.
By using the \mm class, we will incur the full cost of a matrix-vector product.
However, using the \mb, we can reduce the cost of matrix-vector products to $O(n)$.
This applies in the case where $1+l+r<<n$, since if $1+l+r\approx n$, there will be no cost savings since the banded matrix will be dense.
Since we are performing fewer computations when calculating matrix-vector products using a \mb, we would expect that the run time of the algorithm is reduced significantly.
Figure \ref{fig:2d-matrix-type-compare-time} clearly shows the benefit of reduced computation.
The straight lines on the log-log scale plot indicate that there is a polynomial relationship between the time The red points represent the 



\iffalse
\subsection{Different Matrices}
For the matrix A dependent on m, the convergence of cg breaks in general.
Since A is not symmetric, the conditions for convergence do not hold.
However, for large values of m, we may still find convergence for the problem.

If m is large, we have "strong" diagonal dominance. 
So we are effectively solving a diagonal system.
b is a constant vector, so x can be approximated by 2.5/a_{ii}.
If m is small, approximately 0, no convergence at all.
How does this play into cg algorithm?

We need symmetry and pos defness for convergence.
May not have set of linearly independent eigenvectors
So the solution will not converge


\fi


\section{Conclusion}

\iffalse
    Conclusion draws together strands of work throughout the report, 
    demonstrating how the results obtained relate to each other and 
    the broader context of the mathematical problem.

Discussion of results in context of problem
Talk about related algorithms
> BiCG
> CG squared
Preconditioning
Parallelisation, optimisations
Results
\fi

\begin{thebibliography}{9}
    
    \bibitem{top10}
        J. Dongarra and F. Sullivan, 
        "Guest Editors Introduction to the top 10 algorithms",
        in Computing in Science \& Engineering, vol. 2, no. 1, pp. 22-23, Jan.-Feb. 2000.
        doi: 10.1109/MCISE.2000.814652
    
    \bibitem{cghist}
        Gene H. Golub and Dianne P. O’Leary,
        "Some History of the Conjugate Gradient and Lanczos Algorithms: 1948–1976",
        SIAM Review 1989 31:1, 50-102 
        
\end{thebibliography}

\clearpage
\appendix

\section{Figures}

\begin{figure}[h]
	\centering
    \includegraphics[width=0.9\textwidth]{figures/2d-matrix-type-compare-iter.eps}
    \caption{Figure}
    \label{fig:2d-matrix-type-compare-iter}
\end{figure}

\begin{figure}[h]
	\centering
    \includegraphics[width=0.9\textwidth]{figures/2d-matrix-type-compare-time.eps}
    \caption{Figure}
    \label{fig:2d-matrix-type-compare-time}
\end{figure}



\section{Tables}

\bgroup
\def\arraystretch{1.1}
\begin{table}[ht!]
\caption{Table 1}
\label{tab:1}
\csvstyle{myTableStyle}{longtable=|C{1.5cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}|,
% table head= \hline $n$ & Iteration count & Execution time \\ \hline,
table head=\hline & \multicolumn{2}{|c|}{\bfseries MMatrix} & \multicolumn{2}{|c|}{\bfseries MBandedMatrix}\\ $n$ &  Execution time & Iteration count & Execution time & Iteration count \\ \hline,
late after line=\\\hline, no head, separator=tab}
\csvreader[myTableStyle]{data/task3.3.4.dat}{1=\ci, 2=\cii, 3=\ciii, 4=\civ, 5=\cv}
{\ci & \cii & \ciii & \civ & \cv}
\end{table}
\egroup

\bgroup
\def\arraystretch{1.0}
\begin{table}[ht!]
\caption{Table 1}
\label{tab:1}
\csvstyle{myTableStyle}{longtable=|C{1.5cm}|C{3cm}|C{3cm}|,
table head= \hline $n$ & Iteration count & Execution time \\ \hline,
late after line=\\\hline, no head, separator=tab}
\csvreader[myTableStyle]{data/task3.2.4.7.dat}{1=\ci, 2=\cii, 3=\ciii}
{\ci & \cii & \ciii}
\end{table}
\egroup



\section{Code Listings}
\label{sec:code}

\lstinputlisting[language=C++,caption=Filename: mvector.h. Mathematical vector implementation in C++.,label=lst:mvector]{src/mvector.h}

\lstinputlisting[language=C++,caption=Filename: mmatrix.h. Mathematical matrix implementation in C++.,label=lst:mmatrix]{src/mmatrix.h}

\lstinputlisting[language=C++,caption=Filename: mbandedmatrix.h. Banded matrix implementation in C++.,label=lst:mbandedmatrix]{src/mbandedmatrix.h}